{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn, os, pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{os.getcwd()}/ecommerceDataset.csv\")\n",
    "df.columns = [\"category\", \"item\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the data: \", df.shape)\n",
    "print(\"Data info:\\n\", df.info())\n",
    "print(\"Data description:\\n\", df.describe().transpose())\n",
    "print(\"Example data:\\n\", df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(\"\\n\\n\")\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df[\"item\"]:\n",
    "    if type(item) != str:\n",
    "        print(type(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = df[\"category\"].unique()\n",
    "print(df[\"item\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(df_no_duplicates[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_no_duplicates[\"item\"].values\n",
    "labels = df_no_duplicates[\"category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_oh_encoder = OneHotEncoder()\n",
    "labels_oh_encoded = labels_oh_encoder.fit_transform(labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, train_size=0.8, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "oov_token = \"<OOV>\"\n",
    "max_length = 200\n",
    "embedding_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, split=\" \", oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(dict(list(word_index.items())[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = keras.preprocessing.sequence.pad_sequences(X_train_tokens, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "X_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_tokens, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value,key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in tokens])\n",
    "\n",
    "print(X_train_padded[2])\n",
    "print(\"------------\")\n",
    "print(decode_tokens(X_train_padded[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, embedding_dim))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(48)))\n",
    "model.add(keras.layers.Dense(48, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(len(np.unique(labels)), activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])#, tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 20\n",
    "logpath = os.path.join(os.getcwd(), 'tensorboard_log', datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "datetime.now()\n",
    "tb = callbacks.TensorBoard(logpath)\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3)\n",
    "history = model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=max_epoch, callbacks=[tb, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test_padded)\n",
    "prediction_index = np.argmax(prediction, axis=1)\n",
    "\n",
    "report = sklearn.metrics.classification_report(y_test, prediction_index)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f\"{os.getcwd()}/saved_models\") is False:\n",
    "    os.mkdir(f\"{os.getcwd()}/saved_models\")\n",
    "\n",
    "if os.path.exists(f\"{os.getcwd()}/saved_models/latest\") is False:\n",
    "    os.mkdir(f\"{os.getcwd()}/saved_models/latest\")\n",
    "\n",
    "latest_folder = f\"{os.getcwd()}/saved_models/latest\"\n",
    "\n",
    "num = 1\n",
    "saved_model_folder = f\"{os.getcwd()}/saved_models/model{num}\"\n",
    "while os.path.exists(saved_model_folder):\n",
    "    num += 1\n",
    "    saved_model_folder = f\"{os.getcwd()}/saved_models/model{num}\"\n",
    "\n",
    "os.mkdir(saved_model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{saved_model_folder}/tokenizer.json\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{saved_model_folder}/label_encoder.json\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, f\"{saved_model_folder}/saved_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "for filename in os.listdir(saved_model_folder):\n",
    "    shutil.copy(os.path.join(saved_model_folder, filename), os.path.join(latest_folder, filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI07_CPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
